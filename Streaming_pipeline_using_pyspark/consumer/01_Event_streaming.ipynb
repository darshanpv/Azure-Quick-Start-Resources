{"cells":[{"cell_type":"markdown","source":["## Demo - Use of Databricks Structured streaming to process streaming data \n### Load the data from Azure event hub to delta lake\n\nThis notebook shows you how to use Databricks notebbok to consume real time event data from Azure event hub.\n\nADLS Gen2 is mounted to store event data to a data lake."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58fcfd27-4dc3-48bd-9358-51e69bff2c77"}}},{"cell_type":"code","source":["#Input parameters\n#mount point path\ndbutils.widgets.text(\"mount_point_path\", \"/mnt/stream-data\")\n#file_system\ndbutils.widgets.text(\"file_system\", \"stream-data\")\n#account_name\ndbutils.widgets.text(\"account_name\", \"dbcoedl\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Input Parameters","showTitle":true,"inputWidgets":{},"nuid":"696c7c90-e515-48d6-bce6-30f3e1f6f25c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Read Parameters\n\nmount_point_path = dbutils.widgets.get(\"mount_point_path\")\nfile_system = dbutils.widgets.get(\"file_system\")\naccount_name = dbutils.widgets.get(\"account_name\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Read Parameters","showTitle":true,"inputWidgets":{},"nuid":"bf529983-b377-4fb9-9ae9-d29e7480a9be"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Mount with access key is not recommonded way\ndbutils.fs.mount(\n  source = \"wasbs://{}@{}.blob.core.windows.net\".format(file_system,account_name),\n  mount_point = mount_point_path,\n  extra_configs = {\"fs.azure.account.key.dbcoedl.blob.core.windows.net\": \"xgFPK3uYt2t0rCRcfkflpq1U0hzBzV9PS73QYJ4UDHy1rPOPwgaGlAuUO/tG5EDuCdmKugk7srdT+AStHfcDJR==\"})"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Mount ADLS Gen2 container","showTitle":true,"inputWidgets":{},"nuid":"2db2f605-42dd-4931-b4b0-32ec2ac2dd32"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Check mount points\ndbutils.fs.mounts()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e09635be-65e4-40ac-93d1-da6e320b2ccf"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Preparation (Set up Event Hub and library installation)\nBefore starting,\n\n- Create Event Hub Namespace resource in Azure Portal\n- Create new Event Hub in the previous namespace\n- Create SAS policy and copy connection string on generated Event Hub entity\n- Install Event Hub library to your cluster\n- Go to Cluster -> Libraries -> Install New and Select Maven. Install \"com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.22\" on \"Maven\" source\n- Insall \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.2\" library in similar way \n- Add following config to Cluster\n\n`spark.mongodb.output.uri mongodb+srv://admin:demo%40PSL@cluster0.s5tuva0.mongodb.net/events_db?retryWrites=true&w=majority`\n`spark.mongodb.input.uri mongodb+srv://admin:demo%40PSL@cluster0.s5tuva0.mongodb.net/events_db?retryWrites=true&w=majority`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ecf9f92d-905d-4aa8-8a55-06003af3dd9d"}}},{"cell_type":"markdown","source":["Read stream from Azure Event Hub as streaming dataframe using `readStream()`.  \nYou must set your namespace, entity, policy name, and key for Azure Event Hub in the following command."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a93f4325-55e4-4733-8aab-7c399d28af96"}}},{"cell_type":"code","source":["# Read Event Hub's stream\nconf = {}\nconf[\"eventhubs.connectionString\"] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\"Endpoint=sb://events-feed.servicebus.windows.net/;SharedAccessKeyName=manage_user_access_policy;SharedAccessKey=EYMfb85RM5wMgBujKH+D+P/MbFb1Auo+BGkgAbWakIJ=;EntityPath=demo-topic\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Event Hubs Configuration","showTitle":true,"inputWidgets":{},"nuid":"f09cc6db-b315-4321-9513-9fcd81d91712"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["read_df = (\n  spark\n    .readStream\n    .format(\"eventhubs\")\n    .options(**conf)\n    .load()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Read stream data","showTitle":true,"inputWidgets":{},"nuid":"ce222c85-9364-41ec-83ec-d78f55c5367d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nclaims_schema = StructType([\n    StructField(\"id\", StringType(), True),\n    StructField(\"customer_name\", StringType(), True),\n    StructField(\"phone_number\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"claim_amount\", IntegerType(), True),\n    StructField(\"type_id\", StringType(), True),\n    StructField(\"status\", StringType(), True)\n])\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Schema Definition","showTitle":true,"inputWidgets":{},"nuid":"375b4146-22a4-40ed-9a58-da42e43f537b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Read the event body\ndecoded_df = read_df.select(from_json(col(\"body\").cast(\"string\"), claims_schema).alias(\"payload\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Apply schema and write stream","showTitle":true,"inputWidgets":{},"nuid":"136cfb95-f181-4c1d-a5c7-33e1df8bf6ca"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["claims_df = decoded_df.withColumn(\"id\", col(\"payload.id\"))\\\n.withColumn(\"customer_name\", col(\"payload.customer_name\"))\\\n.withColumn(\"phone_number\", col(\"payload.phone_number\"))\\\n.withColumn(\"country\", col(\"payload.country\"))\\\n.withColumn(\"claim_amount\", col(\"payload.claim_amount\"))\\\n.withColumn(\"type_id\", col(\"payload.type_id\"))\\\n.withColumn(\"status\", col(\"payload.status\"))\\\n.drop(\"payload\")\n\nclaims_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Get payload into Claims dataframe","showTitle":true,"inputWidgets":{},"nuid":"f3cc8f33-6a6e-4215-a62a-b413ec270469"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(claims_df, processingTime = \"5 seconds\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Show streaming progress every 5 sec","showTitle":true,"inputWidgets":{},"nuid":"44acd36a-0eea-489e-aec5-0af86864c08f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#data enahancement\nclaims_df = claims_df.withColumn(\"processed\", current_timestamp())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Data enhancement / Transformation","showTitle":true,"inputWidgets":{},"nuid":"f572a2c0-7b86-48d3-bc44-84e58d5ad51d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["claims_types_df = spark.read.format(\"delta\").table(\"events_db.insurance_types\")\ndisplay(claims_types_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Load claim types to dataframe","showTitle":true,"inputWidgets":{},"nuid":"7c98eb3f-e0be-44d1-83ab-f31016c2ee1d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["claims_df=claims_df.join(claims_types_df, claims_df.type_id==claims_types_df.id, \"inner\").drop(claims_types_df.id)\ndisplay(claims_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Join two data frames (facts and dimension tables)","showTitle":true,"inputWidgets":{},"nuid":"0728e6ef-03dc-46fc-bdd1-af2f5cd78f44"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["For real IoT or Sales data stream , you would drop duplicates, do aggregation using `window` function etc.  \nAs an example,\n```\ndef aggregateSalesREvenue(df,watermarkLateness,timeWindowSize,aggregationKey):\n  return (\n  df.withWatermark(\"timestamp\", watermarkLateness)\n  .groupBy(\n    window(\"timestamp\", timeWindowSize),\n    col(aggregationKey))\n  .agg(sum(col(\"sales\")).alias(\"sales\")))\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e35cfba6-c975-4ddc-abe8-84519e2bdc80"}}},{"cell_type":"markdown","source":["#### Write the datastream to Delta Table for Data Analysis\n\nWe will create a database and store the stream data as delta table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7283c4ee-8e2c-4f2b-915d-c52afe0650a3"}}},{"cell_type":"code","source":["%sql\nCREATE DATABASE IF NOT EXISTS events_db;\nUSE events_db;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Create Database","showTitle":true,"inputWidgets":{},"nuid":"400ebf07-d7ee-42bf-8558-2dfe430858fc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["check_point_path = \"dbfs:/FileStore/events/_checkpoints/event_stream\"\n\ndelta_write_query = claims_df.writeStream\\\n.format(\"delta\")\\\n.outputMode(\"append\")\\\n.option(\"checkpointLocation\", check_point_path)\\\n.queryName(\"delta_write_query\")\\\n.toTable(\"claims_data\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Write data stream to a delta table","showTitle":true,"inputWidgets":{},"nuid":"18bd5957-e2c7-46d8-aa5c-d822447c52c8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT * FROM claims_data LIMIT 10;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90a03b2c-f8d9-4d89-a94d-adf537e38f0c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT count(*) FROM claims_data;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06822ddb-2ed3-4b92-88a2-977767662263"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We start streaming computation by defining the sink as streaming query named \"data_lake_query\".  \n`start()` function kicks off the streaming and continue to run as background jobs ..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a5365fc7-b330-494d-9471-385ae3e39979"}}},{"cell_type":"code","source":["save_loc = \"/mnt/stream-data/claims\"\n\ndatalake_write_query = claims_df.writeStream\\\n.format(\"csv\")\\\n.outputMode(\"append\")\\\n.queryName(\"data_lake_query\")\\\n.trigger(processingTime='30 seconds')\\\n.option(\"checkpointLocation\", f\"{save_loc}/_checkpoint\")\\\n.start(save_loc)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Write data stream to Data lake (Capture Events as backup)","showTitle":true,"inputWidgets":{},"nuid":"40485cd7-b17c-4f55-ad2b-b44ab99d23e8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["After completed, cancel (stop) previous jobs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d19fcb2-ff31-422f-9ae9-ccc1ab9af665"}}},{"cell_type":"code","source":["for s in spark.streams.active:\n    s.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Stop streaming","showTitle":true,"inputWidgets":{},"nuid":"05ccc81f-d494-4378-b322-ed5d9d67944e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Unmount\ndbutils.fs.unmount(\"/mnt/stream-data\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Cleanup - Unmount the ADLS Gen2","showTitle":true,"inputWidgets":{},"nuid":"0863a26a-e6c5-45a0-9676-54cc3fda55cc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### ==== end of notebook ===="],"metadata":{"application/vnd.databricks.v1+cell":{"title":"End","showTitle":true,"inputWidgets":{},"nuid":"ed8a3b8d-49c4-41bb-ac0e-6c72180feda2"}}},{"cell_type":"code","source":["%sh\ncd /dbfs/FileStore/events/_checkpoints/event_stream\nrm -rf *"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"713edc26-0c87-4db1-af6a-17474448f8f5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6296556a-8775-43bd-9174-d5ec14cc20bc"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"01_Event_streaming","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":1530943782484789,"dataframes":["_sqldf"]}},"language":"python","widgets":{"mount_point_path":{"nuid":"d571c363-fae8-44c0-ad81-68397e3ef0db","currentValue":"/mnt/stream-data","widgetInfo":{"widgetType":"text","name":"mount_point_path","defaultValue":"/mnt/stream-data","label":null,"options":{"widgetType":"text","validationRegex":null}}},"file_system":{"nuid":"0d7f8cf8-7e4b-4e8e-8ee2-64436df0928d","currentValue":"stream-data","widgetInfo":{"widgetType":"text","name":"file_system","defaultValue":"stream-data","label":null,"options":{"widgetType":"text","validationRegex":null}}},"account_name":{"nuid":"3c7e5917-9810-4921-b187-bbe1f3648359","currentValue":"dbcoedl","widgetInfo":{"widgetType":"text","name":"account_name","defaultValue":"dbcoedl","label":null,"options":{"widgetType":"text","validationRegex":null}}}},"notebookOrigID":1945685863775717}},"nbformat":4,"nbformat_minor":0}
